import os
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from dotenv import load_dotenv
from youtube_transcript_api import YouTubeTranscriptApi

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializar o modelo de chat
chat = ChatGroq(
    api_key=os.getenv('GROQ_API_KEY'), # Chave de API
    model='llama-3.3-70b-versatile', # Modelo LLM a ser usado
    temperature=0.2, # Baixa temperatura para respostas mais precisas
    max_tokens=500 # Limite de tokens na resposta
)

def obter_transcricao_youtube(links):
    """Obt√©m a transcri√ß√£o de v√≠deos do YouTube a partir de uma lista de links."""
    # Inicializa a vari√°vel que ir√° armazenar as transcri√ß√µes concatenadas
    documento = ''
    for link in links:
        try:
            # Extrai o ID do v√≠deo a partir do link fornecido
            video_id = link.split('v=')[-1].split('&')[0]
            # Obt√©m a transcri√ß√£o para os idiomas portugu√™s e ingl√™s
            transcricao = YouTubeTranscriptApi.get_transcript(video_id, languages=['pt', 'en'])
            for trecho in transcricao:
                # Concatena os textos da transcri√ß√£o em um √∫nico documento
                documento += trecho['text'] + ' '
        except Exception as e:
            # Exibe um aviso se ocorrer um erro ao processar o v√≠deo
            st.warning(f'Erro ao processar o v√≠deo: {link}. Erro: {e}')
    return documento

def obter_base_vetores_dos_textos(texto):
    """Divide o texto em peda√ßos e cria uma base vetorial."""

    # Configura o divisor de texto em peda√ßos
    divisor_texto = CharacterTextSplitter(
        separator=' ', # Define o espa√ßo como separador
        chunk_size=500, # Define o tamanho de cada peda√ßo de texto
        chunk_overlap=200, # Define a sobreposi√ß√£o entre os peda√ßos
        length_function=len # Usa o comprimento do texto para controle de tamanho
    )

    # Divide o texto do documento em peda√ßos
    pedacos_documento = divisor_texto.split_text(texto)

    # Configura o modelo de embeddings para gerar representa√ß√µes vetoriais
    modelo_embeddings = HuggingFaceEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2', # Nome do modelo de embeddings
        model_kwargs={'device': 'cpu'}, # Define o uso do CPU para processamento
        encode_kwargs={'normalize_embeddings': False} # N√£o aplica normaliza√ß√£o aos embeddings
    )

    # Cria uma base vetorial persistente usando os textos em peda√ßos
    base_vetores = FAISS.from_texts(pedacos_documento, modelo_embeddings)
    return base_vetores

def montar_prompt(fragmentos, pergunta):
    """Monta manualmente o prompt com os fragmentos e o hist√≥rico de conversa."""

    template = """
    Use os trechos fornecidos para responder √† pergunta do usu√°rio de forma clara e concisa.
    Se necess√°rio, complemente a resposta utilizando o hist√≥rico do chat.
    Se n√£o souber a resposta com base nos trechos fornecidos e no hist√≥rico do chat, diga que n√£o sabe, sem tentar adivinhar ou inventar informa√ß√µes.
    Se poss√≠vel, seja direto e objetivo ao responder.

    ### Trechos:
    {fragmentos}

    ### Pergunta:
    {pergunta}
    """

    # Juntar todos os fragmentos em um √∫nico texto
    contexto = '\n'.join([f'{indice}. {fragmento.page_content}\n' for indice, fragmento in enumerate(fragmentos,1)])

    # Criar e formatar o prompt
    prompt = template.format(fragmentos=contexto, pergunta=pergunta)

    return prompt

def main():
    """Fun√ß√£o principal para configurar e executar a interface da aplica√ß√£o Streamlit."""
    # Inicializa o hist√≥rico de chat na sess√£o, se ainda n√£o existir
    if 'historico_chat' not in st.session_state:
        st.session_state.historico_chat = []
    # Inicializa a base de vetores na sess√£o, se ainda n√£o existir
    if 'base_vetores' not in st.session_state:
        st.session_state.base_vetores = None

    # Configura o t√≠tulo e o √≠cone da p√°gina
    st.set_page_config(page_title='Chat com v√≠deos do YouTube', page_icon='ü§ñ')
    st.title('Chat com v√≠deos do YouTube')

    # Configura a barra lateral para upload de arquivo
    with st.sidebar:
        # Cabe√ßalho das configura√ß√µes
        st.header('üîó Links de V√≠deos')
        # Permite colar URLs de v√≠deos do YouTube
        links_videos = st.text_area(
            'Cole aqui os links dos v√≠deos do YouTube',
            help='Apenas v√≠deos p√∫blicos com transcri√ß√µes ativadas s√£o suportados.'
        )
        if st.button('Processar V√≠deos', use_container_width=True):
            # Mostra spinner durante processamento
            with st.spinner('Processando v√≠deos...'):
                # Inicializa o hist√≥rico de chat com a primeira mensagem do bot
                st.session_state.historico_chat.append(AIMessage(content='Ol√°, sou um bot. Como posso ajudar?'))
                # Obt√©m o texto consolidado das transcri√ß√µes
                texto_completo = obter_transcricao_youtube(links_videos.splitlines())
                # Gera a base vetorial
                st.session_state.base_vetores = obter_base_vetores_dos_textos(texto_completo)
            st.success('V√≠deos processados com sucesso!')

    # Se a base vetorial existir, permite intera√ß√£o no chat
    if st.session_state.base_vetores is not None:
        # Captura a entrada do usu√°rio no chat
        pergunta = st.chat_input('Digite sua mensagem aqui...')
        # Processa a mensagem do usu√°rio e gera resposta
        if pergunta is not None and pergunta != '':
            # Recuperar documentos relevantes com base na pergunta usando o banco vetorial
            documentos_relevantes = st.session_state.base_vetores.similarity_search(pergunta, k=3)

            # Montar o prompt com os fragmentos
            prompt = montar_prompt(documentos_relevantes, pergunta)

            # Adiciona o prompt com os trechos e a pergunta ao hist√≥rico
            st.session_state.historico_chat.append(HumanMessage(content=prompt))

            # Exibir um spinner enquanto o modelo gera a resposta
            with st.spinner('Gerando resposta...'):
                resposta = chat.invoke(st.session_state.historico_chat) # Obter a resposta do modelo

            # Limpa o hist√≥rico antes de adicionar a resposta, removendo o prompt montado
            st.session_state.historico_chat.pop() # Remove a √∫ltima, que seria o prompt montado
            st.session_state.historico_chat.append(HumanMessage(content=pergunta)) # Adiciona apenas a pergunta ao hist√≥rico
            st.session_state.historico_chat.append(AIMessage(content=resposta.content)) # Adicionar a resposta do modelo ao hist√≥rico de mensagens

        # Exibe o hist√≥rico do chat na interface
        for mensagem in st.session_state.historico_chat:
            if isinstance(mensagem, AIMessage): # Mensagem do chatbot
                with st.chat_message('ai'):
                    st.write(mensagem.content)
            elif isinstance(mensagem, HumanMessage): # Mensagem do usu√°rio
                with st.chat_message('human'):
                    st.write(mensagem.content)

# Executa a aplica√ß√£o se o script for chamado diretamente
if __name__ == '__main__':
    main()